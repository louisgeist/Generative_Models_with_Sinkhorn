{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "AqZTSgbfKrLw",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AqZTSgbfKrLw",
    "outputId": "a0adb8a3-4f5e-4833-8a9c-d2fa9b36e353"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/training_folder\n",
      "Cloning into 'Generative_Models_with_Sinkhorn'...\n",
      "remote: Enumerating objects: 45, done.\u001b[K\n",
      "remote: Counting objects: 100% (45/45), done.\u001b[K\n",
      "remote: Compressing objects: 100% (34/34), done.\u001b[K\n",
      "remote: Total 45 (delta 20), reused 30 (delta 11), pack-reused 0\u001b[K\n",
      "Receiving objects: 100% (45/45), 12.17 KiB | 12.17 MiB/s, done.\n",
      "Resolving deltas: 100% (20/20), done.\n",
      "/content/training_folder/Generative_Models_with_Sinkhorn\n"
     ]
    }
   ],
   "source": [
    "!mkdir training_folder && cd training_folder\n",
    "%cd training_folder\n",
    "!git clone https://github.com/louisgeist/Generative_Models_with_Sinkhorn.git\n",
    "%cd Generative_Models_with_Sinkhorn/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "UR1hJOI_OxLd",
   "metadata": {
    "id": "UR1hJOI_OxLd"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from gen_model_with_sinkhorn import Model\n",
    "\n",
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9d99d902",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"mps:0\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "device = torch.device(\"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8ea3ad5a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8ea3ad5a",
    "outputId": "e749cf1d-01b6-41d1-d49f-cdd2e2854f08"
   },
   "outputs": [],
   "source": [
    "#PARAMETERS\n",
    "batch_size = 32\n",
    "epochs = 20\n",
    "generator_dim = [[2,32], [32,128], [128, 784]] #last one should be [_,784]\n",
    "learned_cost_dim = [[784, 128], [128, 128]] #first one should be [784, _]\n",
    "lr = 0.01\n",
    "learnable_cost = False\n",
    "epsilon = 0.1\n",
    "\n",
    "model = Model(generator_dim, learned_cost_dim, batch_size, lr, epsilon, learnable_cost, device)\n",
    "\n",
    "#to use normalized version of MNIST\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,)) \n",
    "])\n",
    "\n",
    "# Download and load the MNIST dataset\n",
    "train_dataset = datasets.MNIST(root='./data', train=True, transform=transform, download=True)\n",
    "test_dataset = datasets.MNIST(root='./data', train=False, transform=transform, download=True)\n",
    "\n",
    "\n",
    "# Flatten 28x28 image into a vector= of 784 fetures\n",
    "flatten = transforms.Lambda(lambda x: x.view(-1))\n",
    "\n",
    "# Apply the flatten transformation to the dataset\n",
    "train_dataset.transform = transforms.Compose([transform, flatten])\n",
    "test_dataset.transform = transforms.Compose([transform, flatten])\n",
    "\n",
    "# Create dataloaders\n",
    "train_dataloader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_dataloader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c46b4c42",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 402
    },
    "id": "c46b4c42",
    "outputId": "3c5b63f5-554f-4d62-f628-80af34849040"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of the epoch due to the end of the for loop on k.\n",
      "Epoch 1 (0.58 s): loss = 1951.2650899887085\n",
      "\n",
      "End of the epoch due to the end of the for loop on k.\n",
      "Epoch 2 (0.57 s): loss = 1754.7034482955933\n",
      "\n",
      "End of the epoch due to the end of the for loop on k.\n",
      "Epoch 3 (0.58 s): loss = 1758.5934581756592\n",
      "\n",
      "End of the epoch due to the end of the for loop on k.\n",
      "Epoch 4 (0.57 s): loss = 1748.366171836853\n",
      "\n",
      "End of the epoch due to the end of the for loop on k.\n",
      "Epoch 5 (0.57 s): loss = 1755.2626008987427\n",
      "\n",
      "End of the epoch due to the end of the for loop on k.\n",
      "Epoch 6 (0.58 s): loss = 1752.4600706100464\n",
      "\n",
      "End of the epoch due to the end of the for loop on k.\n",
      "Epoch 7 (0.57 s): loss = 1745.0420379638672\n",
      "\n",
      "End of the epoch due to the end of the for loop on k.\n",
      "Epoch 8 (0.58 s): loss = 1759.1107063293457\n",
      "\n",
      "End of the epoch due to the end of the for loop on k.\n",
      "Epoch 9 (0.57 s): loss = 1739.1436014175415\n",
      "\n",
      "End of the epoch due to the end of the for loop on k.\n",
      "Epoch 10 (0.58 s): loss = 1738.830470085144\n",
      "\n",
      "End of the epoch due to the end of the for loop on k.\n",
      "Epoch 11 (0.58 s): loss = 1726.0647583007812\n",
      "\n",
      "End of the epoch due to the end of the for loop on k.\n",
      "Epoch 12 (0.57 s): loss = 1736.7376699447632\n",
      "\n",
      "End of the epoch due to the end of the for loop on k.\n",
      "Epoch 13 (0.57 s): loss = 1753.8867778778076\n",
      "\n",
      "End of the epoch due to the end of the for loop on k.\n",
      "Epoch 14 (0.58 s): loss = 1720.2005167007446\n",
      "\n",
      "End of the epoch due to the end of the for loop on k.\n",
      "Epoch 15 (0.57 s): loss = 1757.4278945922852\n",
      "\n",
      "End of the epoch due to the end of the for loop on k.\n",
      "Epoch 16 (0.57 s): loss = 1733.9904146194458\n",
      "\n",
      "End of the epoch due to the end of the for loop on k.\n",
      "Epoch 17 (0.62 s): loss = 1752.1795415878296\n",
      "\n",
      "End of the epoch due to the end of the for loop on k.\n",
      "Epoch 18 (0.58 s): loss = 1737.8182229995728\n",
      "\n",
      "End of the epoch due to the end of the for loop on k.\n",
      "Epoch 19 (0.57 s): loss = 1737.6569080352783\n",
      "\n",
      "End of the epoch due to the end of the for loop on k.\n",
      "Epoch 20 (0.57 s): loss = 1735.7690238952637\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# training loop\n",
    "start = time.time()\n",
    "for epoch in range(1,epochs+1):\n",
    "    model.train(True)\n",
    "    loss = model.train_1epoch(train_dataloader)\n",
    "    end = time.time()\n",
    "    print(f\"Epoch {epoch} ({round(end-start,2)} s): loss = {loss}\\n\")\n",
    "    start = end\n",
    "    \n",
    "\n",
    "torch.save(model, \"basic_model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "RGS2GjgeMqXO",
   "metadata": {
    "id": "RGS2GjgeMqXO"
   },
   "outputs": [],
   "source": [
    "import visualization"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
